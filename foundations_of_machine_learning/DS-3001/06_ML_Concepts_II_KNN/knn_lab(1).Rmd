---
title: "KNN Lab"
author: "Brian Wright"
date: "4/7/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

You are a new data scientist and unfortunately have a demanding and totally clueless boss.  Clueless meaning that he doesn't understand data science, but he knows he wants it to be used to fix all the company's problems and you are just the data scientist to do it! 

Your company, Marketing Enterprises of Halifax or "MEH" is being beat out by the competition and wants a new way to determine the quality of its commercials. Your boss, Mr. Ed Rooney, would like the company's commercials to seem more like actual TV shows. So we wants you to develop a "machine learning thing" using the companyâ€™s internal data to classify when something is a commercial and when it is not. Mr. Rooney believes the company will then know how to trick potential future customers into thinking their commercials are actually still part of the show and as a result will pay more attention and thus buy more of the terrible products "MEH" is supporting (it's a terrible plan, but you have to make a living). 

Given that MEH is producing commercials more or less continuously you know there 
will be a need to update the model quite frequently, also being a new data scientist and having a clueless boss you decide to use a accessible approach that you might be able to explain to Mr. Rooney, (given several months of dedicated one on one time), that approach is k-nearest neighbor. 

You'll also need to document your work extensively, because Mr. Rooney doesn't know he's clueless so he will ask lots of "insightful" questions and require lots of detail that he won't understand, so you'll need to have an easy to use reference document. Before you get started you hearken back to the excellent education you received at UVA and using this knowledge outline roughly 20 steps that need to be completed to build this algo for MEH and Ed, they are documented below...good luck.
 

```{r}
#1
#Load in the data, both the commercial dataset and the labels. You'll need to the place the labels on the columns. The dataset "tv_commercialsets-CNN_Cleaned.csv",  is data collected about the features of commercials on CNN. We can try to predict what segments of video are commercials based on their audio and video components. More information on the datasets can be found data.world:

# https://data.world/kramea/tv-commercial-detection/workspace/file?filename=tv_commercial_datasets%2FBBC_Cleaned.csv

#You can use the function colnames() to apply the labels (hint: you might need to reshape the labels to make this work)

```

```{r}
#2. Determine the split between commerical and non-commercial then calculate the base rate, assume 1 is the commercial label and -1 is the non-commercial label 
```

```{r}
#3.  Since there are columns that contain different metrics for
#    the same variable (i.e. any column that ends in 'mn' is the
#    mean of that variable, while any column that ends in 'var' is
#    the variance of that variable), we don't need to keep both, drop all the 
#    columns that include var
```

```{r}
#4.  Before we run knn, sometimes it's good to check to make sure that our variables
#    are not highly correlated. Use the cor() function on
#    'your_dataframe', label it 'commercial_correlations', and
#    view the data.

```

```{r}
#5. Determine which variables to remove, high correlations start around .7 or below -.7 I would especially remove variables that appear to be correlated with more than one variable. List your rationale here:
```

```{r}
#6. Subset the dataframe based on above.
```


```{r}
#7. Now we have our data and are ready to run the KNN, but we need to split into test and train. Create a index the will divide the data into a 70/30 split
```

```{r}
#8. Use the index above to generate a train and test sets, then check the row counts to be safe and show Mr. Rooney. 
```

```{r}
#9 Train the classifier using k = 3, remember to set.seed so you can repeat the output and to use the labels as a vector for the class (not a index of the dataframe)
```


```{r}
#10 Check the output using str and length just to be sure it worked
```

```{r}
#11 Create a initial confusion matrix using the table function and pass it to a object. (xx <- your confusion matrix)
```

```{r}
#12 Select the true positives and true negatives by selecting
#     only the cells where the row and column names
#     are the same.
```

```{r}
#13 Calculate the accuracy rate by dividing the correct 
#     classifications by the total number of classifications.
#     Label the data 'kNN_acc_com', and view it. Comment on how this compares to
#     the base rate. 
```

```{r}
#14  Run the confusion matrix function and comment on the model output
```

```{r}
#15 Run the "chooseK" function to find the perfect K, while using sapply() function on chooseK() to test k from 1 to 21 (only selecting the odd numbers), and set
#     the train_set argument to 'commercial_train', val_set
#     to 'commercial_test', train_class to the "label" 
#     column of 'commercial_train', and val_class to the
#     "label" column of 'commercial_test'. Label this
#     "knn_diff_k_com"
```

```{r}
#16 Create a data frame so we can visualize the
#     difference in accuracy based on K, convert the matrix to a dataframe
```

```{r}
#17 Use ggplot to show the output and comment on the k to select
```

```{r}
#18 Rerun the model  with "optimal" k 
```

```{r}
#19 Use the confusion matrix function to measure the quality of the new model
```

```{r}
#20 Summarize the differences in language Mr. Rooney may actually understand. Include a discussion on which approach k=3 or k=optimal is the better method moving forward for "MEH". 
```

