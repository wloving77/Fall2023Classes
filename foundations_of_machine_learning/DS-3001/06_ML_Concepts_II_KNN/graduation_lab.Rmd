---
title: "KNN_lab_2"
author: "Brian Wright"
date: "8/17/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(mice)
```

Instructions:
Let's build a kNN model using the college completion data or job placement from last week.

The data is messy and you have a degrees of freedom problem, as in, we have too many features.  

You've done most of the hard work already, so you should be ready to move forward with building your model. 

Use the question/target variable you submitted from last week and build a model to answer the question you created for this dataset. 

Build and optimize a kNN model to predict your target variable. Meaning use the tune set to select the correct k value. 

Experiment with the threshold function, what happens at higher and lower thresholds. Document what you see in comments. 

Evaluate the results using the confusion matrix (at the default threshold). Then talk through your question, summarize what concerns or positive elements do you have about the model? 


Bonus: Adjust the function that selects k to output on Specificity instead of Accuracy

Example of how I cleaned the dataset
```{r}
url <- "https://query.data.world/s/yd5wiazzlj7aahmn4x37y7zq5pyh2h"

grad_data <- read_csv(url)

View(grad_data)# we can tell that a large number of the features will likely 
#need to be removed and we've got a rather large number of features/ 

# readme for the dataset - https://data.world/databeats/college-completion/workspace/file?filename=README.txt


#This will allow us to pull column numbers for sub-setting  
column_index <- tibble(colnames(grad_data))

print(column_index, n=62)#input the n=62 to be able to see all the features

#Use the mice package to track the missing values

#Why are we not seeing missing values? 
md.pattern(grad_data, rotate.names = TRUE)

#replace the null with NA
grad_data[grad_data=="NULL"] <- NA

md.pattern(grad_data, rotate.names = TRUE)

colSums(is.na(grad_data))#?colSums
print(column_index, n=62)

x <- 40:56 #create a list so we don't have to type 16 numbers out

#Most of these columns have a good number of missing values or are not useful.  
grad_data_1 <- grad_data[ ,c(-10,-11,-12,-x,-28,-29,-37,-57,-61)]

#Make a new index
column_index_1 <- tibble(colnames(grad_data_1))
colSums(is.na(grad_data_1))
print(column_index_1, n=37)

#Looking better
View(grad_data_2)

#Dropped a bunch more that appeared to be repeats or not predictive, this time using the the names and subset/select 
grad_data_2 <- subset(grad_data_1, select = -c(unitid,city,state,basic,med_sat_value,med_sat_percentile,counted_pct))

summary(grad_data_2)
colSums(is.na(grad_data_2))

#In looking at the HBCU (historically black colleges and universities, seems
# like the NAs should be 0s, let's change that back and convert to a factor)
grad_data_2$hbcu <- as.factor(ifelse(is.na(grad_data_2$hbcu),0,1))
#same for flagship
grad_data_2$flagship <- as.factor(ifelse(is.na(grad_data_2$flagship),0,1))

#Ok better
table(grad_data_2$hbcu)
str(grad_data_2)

#convert several variables to factors 
x <- c("level","control")
grad_data_2[,x] <- lapply(grad_data_2[,x], as.factor)

#convert several variables to numbers 
x <- c("ft_fac_percentile", "grad_100_value","grad_100_percentile","grad_150_value","grad_150_percentile","retain_value","cohort_size","ft_fac_value")

grad_data_2[,x] <- lapply(grad_data_2[,x], as.numeric)

#Looking better
str(grad_data_2)
```

### Missing Data 
```{r}
#Now let's take a look at missing data issue
md.pattern(grad_data_2, rotate.names = TRUE)

#Delete the rest of the NA columns and drop the college names 
grad_data_2 <- grad_data_2[,-1]
grad_data_3 <- grad_data_2[complete.cases(grad_data_2), ]

str(grad_data_3)#Ok looking good, still likely want to drop a few columns,need to normalize and one_hot encode before we move forward with model building.

md.pattern(grad_data_3, rotate.names = TRUE)

str(grad_data)

md.pattern(grad_data_3)
```
