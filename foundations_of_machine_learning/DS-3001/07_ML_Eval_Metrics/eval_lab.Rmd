---
title: "Eval_Lab"
author: "Brian Wright"
date: "10/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The most important part of any machine learning model(or any model, really) is understanding and defining the models weaknesses and/or vulnerabilities. 

To do so we are going to practice on a familiar dataset and use a method we just learned, kNN. For this lab use ### find a new dataset.  

Part 1. Develop a ML classification question and choose a target variable. In consideration of all the metrics we discussed what are a couple of key metrics that should be tracked given the question you are trying to solve?

Part 2. Create a kNN model that can answer your question, using all the appropriate prep methods you have been using in class discussed.   

Part 3. Evaluate the model using the metrics you identified in the first question. Make sure to calculate/reference the prevalence to provide a baseline for some of these measures. Summarize the output of the key metrics you established in part 1. 

Part 4.  Consider where miss-classification errors (via confusion matrix) are occurring, is there a pattern? If so discuss this pattern and why you think this is the case. 

Part 5. Based on your exploration in Part 4, change the threshold using the function provided in the in-class example, what differences do you see in the evaluation metrics? Speak specifically to the metrics that are best suited to address the question you are trying to answer from part 1. 

Part 6. Summarize your findings (a paragraph or two) speaking through your question, what does the evaluation outputs mean when answering the question you've proposed?

Submit a .Rmd file along with the data used or access to the data sources to the Canvas site. You can work together with your groups but submit individually and generate your own code file. 

