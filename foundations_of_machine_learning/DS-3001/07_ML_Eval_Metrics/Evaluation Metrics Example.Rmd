---
title: "Evaluation Metrics"
author: "Brian Wright"
date: "3/31/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r}
library(tidyverse)
library(caret)
library(RColorBrewer)
library(ROCR)
#install.packages("MLmetrics")
library(MLmetrics)
library(mltools)
library(data.table)

```

## Learning Outcomes 
  * Understand the importance of model evaluation
  * Understand that the selection of a model evaluation metric is related to your question. 
  * Evaluation Metrics can be misleading, meaning using several metrics at any one time is best practices 
  * Exploration of different approaches to ML evaluation



## Loading and cleaning data

I should note that the labels on this dataset were completely anatomized so I had to guess (make up some of my own) at the labels. In terms of walking you through the process of exploring evaluation it really shouldn't have any impact. 

```{r}
#Let's load in our data
loan_data <- read_csv('data/japan_credit.csv')

#Take a look
#getwd()

#str(loan_data)

#placement <-read.csv('data/placement.cvs')


table(loan_data$gender)# so this is kinda tricky we have "?" in the data instead of NAs, so we should probably replace the "?" with NAs, then remove them, if we rerun the table command we can see the "?" are removed. 

dim(loan_data)

table(is.na(loan_data))#Using this function to count and table NAs, does not look like there are any (690x15=10,350)

#Replace the ? with NA
loan_data[loan_data=="?"] <- NA

#Now we see weve got about 67
table(is.na(loan_data))


loan_data_2 <- loan_data[complete.cases(loan_data), ]#We can run some quick analysis to see how much loss we would incur if we just subset to rows with no NAs
loan_data_2

table(is.na(loan_data_2))

dim(loan_data)#original dataset
dim(loan_data_2)#we didn't loose that many case, looks like about 37 (~5%), actually going to overwrite to loan_data

loan_data <- loan_data[complete.cases(loan_data), ]#ok makes the naming simpler, keeping the original data name
dim(loan_data)

#Ok as you can see we have 15 predictor variables (including whether the applications prefer funfetti_cake) and one outcome measure. We do need to re-class the outcome measure to the traditional {0,1} format along with a few other variables, we can do this with recode.
str(loan_data)

loan_data$outcome <- recode(loan_data$outcome, 
                            '+' = 1, 
                            '-' = 0)

#need to do the same for gender, more b than a so b will be male and a female 

loan_data$gender <- recode(loan_data$gender, 
                           'b' = 'm', 
                           'a' = 'f')

#also going to recode marital_status g=married, p= divorced, s=single

loan_data$marital_status <- recode(loan_data$marital_status, 'g' = 'mar', 'p' = 'div', 's'='sig') 

#need to refactor the race variable
table(loan_data$race)#first we can use table to see the frequencies at each of the categories. Given that this is Japanese data we will classify v and Jap, h as white, bb as black, ff as Hispanic and everything else into other. 

loan_data$race <- fct_collapse(loan_data$race,
                               asian = "v",
                               white ="h",
                               black = "bb",
                               hisp = "ff",
                               other = c("z","o","n","j","dd"))

table(loan_data$race)#run table again we see our collapsed categories 
                               
#also age and days_account_open need to be a numeric variable and outcome should be a factor so need to do some quick variable class changes 
 
loan_data$age <- as.numeric(loan_data$age)
loan_data$days_account_open <-as.numeric(loan_data$days_account_open)

str(loan_data)
  
#Next let's get rid of var_d as it's essential the same as var_e and also f as its got 15 different categories so the complexity of keeping that many levels would be pretty difficult to manage inside our tree, especially not knowing the labels.  

loan_pred <- loan_data[ ,-c(4:6)] 
str(loan_pred)

```

## One-Hot Encode and Normalizing
```{r}
column_index <- tibble(colnames(loan_pred))

str(loan_pred)
column_index

loan_pred[,c(1,6,8,9,12)] <- lapply(loan_pred[,c(1,6,8,9,12)], as.factor)

View(loan_pred)

str(loan_pred)


loan_pred <- one_hot(as.data.table(loan_pred),cols=c('race','marital_status'),sparsifyNAs = TRUE,naCols = FALSE,dropCols = TRUE,dropUnusedLevels = TRUE)

str(loan_pred)#All 1s and 0s now we need to normalize

abc <- names(select_if(loan_pred, is.numeric))

loan_pred <- as.tibble(loan_pred)#converting back to a tibble for the normalizing function...what does the warning mean

#Need to load/run the function from the ML Bootcamp
loan_pred[abc] <- lapply(loan_pred[abc], normalize)

str(loan_pred)

```
 
## Data Partition 
```{r}
#We need to create index that we can use for developing a test and training set. Training is for building the tree and test is for checking the quality of the model. 

set.seed(1984)# this will allow you to replicate the outcomes of randomized process

#caret function the will allow us to divide the data into test and train, it will randomly assign rows into each category while maintaining the relative balance (0 and 1s) of the target variable. 
split_index <- createDataPartition(loan_pred$outcome, p = .8, #selects the split, 80% training 20% for test 
                                  list = FALSE,#output of the data, we don't want a list
                                  times = 1)#the number of partitions to create we just want one


#then we just pass the index to our dataset

train_data <- loan_pred[split_index,]
dim(train_data)


test <- loan_pred[-split_index,]
dim(test)

```

## Build a decision tree (use kNN for your lab)
```{r}
#implementing a parallel processes structure, with the 4 cores available on my CPU. 

#install.packages("foreach")#library that caret uses to do implement the processing 

#install.packages("doParallel")#package that registers the cores
#library(doParallel)

#cl <- makePSOCKcluster(4)#registers the cores

#registerDoParallel(cl)

set.seed(1984)
loan_tree <- train(outcome~., #model formula everything used to classify outcome
                   data=train_data, #use the training data
                   method='rpart',# indicates the use of tree based model
                   na.action = na.omit)#omitting the missing values

#stopCluster(cl)


                   
loan_tree #let's take a look, pretty good, accuracy is at roughly 84%, not bad. Accuracy is (TP + TN)/(TP+TN+FP+FN). High level indicator of model efficiency. 

loan_tree
#Quick overview of how bootstrapping works. 
xx <- tibble(loan_tree$resample)
View(xx) 

mean(xx$Accuracy)

loan_tree$finalModel$variable.importance#This will tell us the most important variables in terms of reducing our model error...hahaha liking funfetti takes the cake!! As it should anyone that doesn't enjoy a nice piece of funfetti cake just can't be trusted. (calculated for each variable individually as the sum of the decrease in impurity, includes as a primary split and when it appears as a surrogate)


coul <- brewer.pal(5, "Set2")
barplot(loan_tree$finalModel$variable.importance, col=coul)#Totally unnecessary but couldn't help myself, funfetti barchart...

loan_tree$finalModel

rpart.plot::rpart.plot(loan_tree$finalModel, type=4,extra=101)
```


## Use the mode to predict and the evaluate 
```{r}
#First we need to do some predictions using the test data 
loan_tree
loan_eval <-(predict(loan_tree,newdata = test))#generates 1s and 0s
head(loan_eval,50)

loan_eval_prob <- predict(loan_tree, newdata = test, type = "prob")#this gives us the predicted prob, we will need these later for the fairness evaluation
View(loan_eval_prob)


table(loan_eval, test$outcome)#essential the confusion matrix, though we can make a fancy one using caret built in functions


confusionMatrix(loan_eval, test$outcome, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")

#from the above we can see our True Positive Rate or sensitivity is quite good @ 94%, False Positive Rate (1-Specificity) is also not terrible ~ @ 18%, we want this to be low. 


#Quick function to explore various threshold levels and output a confusion matrix

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

densityplot(loan_eval_prob$`1`)#Why does this look the way it does? 



adjust_thres(loan_eval_prob$`1`,.20, test$outcome) #Not much changes here because of the high probability splits of the data outcomes. Let's take a closer look. We can see that the algo isn't marginally mis-classifying these rows, it's very confidently wrong. Which likely means that there's too much emphasis being placed on too small a number of variables, principally the funfetti variable. 
loan_eval_prob$test <- test$outcome

View(loan_eval_prob)

```

### ROC/AUC
```{r}
#In order to use most evaluation packages it's just easier to have are predictions and targets in one place. 
View(loan_eval)
loan_eval <- tibble(pred_class=loan_eval, pred_prob=loan_eval_prob$`1`,target=test$outcome)


View(loan_eval)

#Function that is part of the ROCR package that preps the data to generate the ROC/AUC curves
pred <- prediction(loan_eval$pred_prob,loan_eval$target)
View(pred)

tree_perf <- performance(pred,"tpr","fpr")
#Essentially creates x and y values in the form of true poss and true neg
#rates at each threshold

View(tree_perf)

plot(tree_perf, colorize=TRUE)
abline(a=0, b= 1)

tree_perf_AUC <- performance(pred,"auc")

print(tree_perf_AUC@y.values)

```

### LogLoss
```{r}

str(loan_eval)

LogLoss(loan_eval$pred_prob, as.numeric(test$outcome))
#We want this number to be rather close to 0, but compare to our baseline which is .45 so this is a fairly bad result, we want this number to be lower than the baseline. 

table(test$outcome)
59/(71+59)#prevalence 

-log(59/130)#baseline 

```

[Logloss Article](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a)

### F1 Score - 
Harmonic Mean of Precision(TP/TP+FP, when the model classifies a positive how often is it correct) and Recall(Sensitivity/TPR: ability of the model to correctly detect the positive class from all options TP/TP+FN).
```{r}
pred_1 <- ifelse(loan_eval_prob$`1` < 0.5, 0, 1)

View(pred_1)
F1_Score(y_pred = loan_eval$target, y_true = loan_eval$pred_class, positive = "1")

#0 to 100 scale

```

## Look at a continous example real quick
Try to predict balance 
```{r}

##unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
#}

#unregister_dopar()cleans up the parallelization in the background

data(trees)
trees

loan_tree <- train(Girth~., #model formula everything used to classify outcome
                   data=trees, #use the training data
                   method='rpart',# indicates the use of tree based model
                   na.action = na.omit)

loan_tree$finalModel

loan_tree_cont <- predict(loan_tree,trees)

loan_tree_cont

rmse <- rmse(loan_tree_cont,trees$Girth)

range(trees$Girth)
20.6-8.3

rmse

rmse/12.3

```

