---
title: "Random_Forest_Lab"
author: "Brian Wright"
date: "11/16/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal of this lab is to optimize a Random Forest model using the provided dataset.  The guidance this week is less prescriptive in terms of steps, so use the skills you have gained over the semester to build and evaluate the RF model. You will be graded on your model building, interpretation of the results and explanation of model selection. As always, rely on your teams but submit your own code. Lastly, there are likely several correct approaches involving a variety of different conclusions, just make sure your conclusions are supported by your approach.    

The dataset should be familiar as it's the census data, on 32,000+ individuals with a variety of variables and a target variable for above or below 50k in salary. 

Your goal is to build a Random Forest Classifier to be able to predict income levels above or below 50k.

Answer the below questions along the way. 

```{r}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

census <- read_csv(url, col_names = FALSE)

colnames(census) <- c("age","workclass","fnlwgt","education","education_num","marital_status","occupation","relationship","race","sex","capital_gain","capital_loss","hours_per_week","native_country","income")


View(census)

```
Below are a few key steps to include but are certainly not everything you will need. 

Remember to review the prevalence/baseline of the target variable to give yourself and idea of well the model is working. (Always start by going through all the necessary steps to prepare the data for ML)

Calculate the initial mtry level 
```{r}

```

Run the initial RF model with 1000 trees 
```{r}

```


Take a look at the variable importance measures, are they what you expected?
```{r}

```

Use the training and tune datasets to optimize the model in consideration of the number of trees, the number of variables to sample and the sample size that optimize the model output. 
```{r}

```

Once a final model has been selected (hyper-parameters of the model are set), evaluate the model using the test dataset. 
```{r}

```

Reduce the size of the dataset using the variable importance measure as a metric for variable selection (reduction the feature space). There's not a set rule here on how to select the variables, so just experiment and track the results. 

```{r}

```

Rebuild the model using the sparse dataset. 

```{r}

```

Summarize your findings. Compare the full model and the sparse model, did they perform the same? Think about not only evaluation measures but in training metrics (no. of trees etc). Which model would you recommend and why? 
```{r}

```
 Bonus - Using this documentation read the section referring to Balancing Prediction Error. Use the overview to balance the errors of the dataset across classes. (+10 points)
 


