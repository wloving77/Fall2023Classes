---
title: "Quick Fairness Output"
author: "Brian Wright"
date: "7/19/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE)
```

```{r, include=FALSE}

#install.packages("rmarkdown")
library(rmarkdown)
load("fairness.RData")
library(plotly)
library(fairness)
library(DT)
library(rpart.plot)
install.packages("rpart")
library(rpart)

```

<center>
Ok let's take a look at some of these fairness measures in action. We are going to train a decision tree on a data set focused on Japanese Loan Approval to see if gender and race play a role in the approval of the loans. 

I should note that the labels on this dataset were completely anonymized so I had to guess (make up some of my own) at the labels. In terms of walking you through the process of measuring fairness it really shouldn't have any impact.  

```{r, echo=FALSE}

rpart.plot(loan_tree$finalModel, type =4, extra = 101)

```

## Demographic Parity 
```{r, echo=FALSE}

#Overall Proportional Probability Density for Gender
prod_plty

#Equity of Odds 
dpp$Metric

ddp_metric_plt



```


## Equality of Opportunity
```{r}
#Essentially the ratio of True Positive Rates (Sensitivity)
eqo_loan$Metric_plot


#We can also look at individual ROCurves
roc_eval <- roc_parity(data = fair_eval_data,
                   outcome = "outcome",
                   group="gender",#protected class
                   probs = "prob",
                   outcome_levels = c("0","1"),
                   base = "m")

#roc_eval$ROCAUC_plot #we would likely want to set our threshold at the intersection of these two graphs, but these seems to be a rather minor difference. 

roc_eval_plty <- ggplotly(roc_eval$ROCAUC_plot)
roc_eval_plty
```


```{r, echo=FALSE}

#Here we are looking at Precision
datatable(prp$Metric, options = list(pageLength = 10))

prp_metric

prp_prob

```

