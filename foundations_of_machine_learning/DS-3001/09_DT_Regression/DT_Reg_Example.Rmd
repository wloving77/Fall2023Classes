---
title: "Trees_Regression_example"
author: "Brian Wright"
date: "3/21/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(ROCR)
library(MLmetrics)
library(mltools)
library(rpart.plot)
```


```{r}
View(winequality)
load("trees_class.Rdata")#loads all the hard work we did last week
#Using the same data set, trying to predict alcohol content
```


## If you need to do all the cleaning again....see below. 
```{r}

winequality <- read.csv("data/winequality-red-ddl.csv")
View(winequality)
str(winequality)
table(winequality$text_rank)

#drop 12 predicts text_rank perfectly

winequality <- winequality[,-12]

sum(is.na(winequality))#got some NAs to deal with here...what should we do?
```


## Missing Data, options, insert clean code or use previous weeks lab
```{r}
summary(winequality)#show the location by variable
#still have a collapsed text_rank which is probably fine

str(winequality)
summary(winequality)
```


## Splitting the Data: 
```{r}
set.seed(1999)
part_index_1 <- caret::createDataPartition(winequality$alcohol,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)

train <- winequality[part_index_1, ]
tune_and_test <- winequality[-part_index_1, ]
train
#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$alcohol,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(test) 
dim(tune)

```

## Let's Build the Model 

```{r}
# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.



features <- train[,-11]#dropping 11 because it's target variable. 
View(features)
target <- train$alcohol

target

str(features)

str(target)
#Three steps in building a caret ML model
#Step 1: Cross validation process-the process by which the training data will be used to build the initial model must be set. As seen below:

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5) 
# number - number of folds
# repeats - number of times the CV is repeated, takes the average of these repeat rounds
#review the documentation on https://topepo.github.io/caret/measuring-performance.htm

#Step 2: Usually involves setting a hyper-parameter search. This is optional and the hyper-parameters vary by model. Let's take a look at the documentation for the model we are going to use. Same search function as for classification 

tree.grid <- expand.grid(maxdepth=c(3:20))

#  2^(k+1)âˆ’1 = maximum number of terminal nodes (splits) when k=depth of the tree
#let's look at the documentation in two places 
# for the tune grid function: https://topepo.github.io/caret/model-training-and-tuning.html

#options for the rpart2: https://topepo.github.io/caret/train-models-by-tag.html#tree-based-model

#Step 3: Train the models
set.seed(1984)
wine_mdl_r <- train(x=features,
                y=target,
                method="rpart2",
                trControl=fitControl,
                metric="RMSE")
wine_mdl_r

wine_mdl_1_r <- train(x=features,
                y=target,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="RMSE")
```

## Let's see how we did. 
```{r}
wine_mdl_r
wine_mdl_1_r

plot(wine_mdl_1_r)
plot(wine_mdl_r)
varImp(wine_mdl_1_r)

rpart.plot(wine_mdl_1_r$finalModel, type=5,extra=101)

wine_mdl_1_r$results
#compared to classifier:

wine_mdl$results

```

## Let's make some prediction and see how we did.
```{r}

wine_pred_tune_r = predict(wine_mdl_1_r,tune)

View(as_tibble(wine_pred_tune_r))

postResample(pred = wine_pred_tune_r, obs = tune$alcohol)
#We want this number, RSME, to be low relative to the range of the target variable and Rsquared to be close to 1. 
range(tune$alcohol)
14-9
.73/5
varImp(wine_mdl_1_r)

#Percentage error in terms of the range of the target variable or NRMSE

#reference for several ways to normalize RMSE, https://www.marinedatascience.co/blog/2019/01/07/normalizing-the-rmse/

```


## Build the tree with depth 5 and Predict with Test Set, anything further is really overfitting   
```{r}

tree.grid_5 <- expand.grid(maxdepth=c(5))

wine_mdl_1_5 <- train(x=features,
                y=target,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid_5,#expanded grid
                metric="RMSE")

rpart.plot(wine_mdl_1_5$finalModel, type = 5, extra=101)

pred_test_reg <- predict(wine_mdl_1_5,test)

head(pred_test_reg)

postResample(pred = pred_test_reg,obs = test$alcohol)

str(target)

```

