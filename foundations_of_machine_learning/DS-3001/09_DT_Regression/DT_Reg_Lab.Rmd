---
title: "Tree_Regression_Lab"
author: "Brian Wright"
date: "3/21/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "In Class DT"
author: "Brian Wright"
date: "December 7, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#This week we are trying to predict a continous variable of your choosing from the below dataset. Follow the steps, mostly same as last week.  

```{r}
# http://archive.ics.uci.edu/ml/datasets/Adult

url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

names <- c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours.per.week","native.country", "salary")

xx <- readr::read_csv(url,col_names=names)

xx[xx=="?"] <- NA

View(xx)

summary(xx)

table(xx$workclass)

histogram(xx$age,type='count', nint=30)

```

```{r}
#1 Load the data and ensure the labels are correct. You are working to develop a model that can predict age.   
```

```{r}
#2 Ensure all the variables are classified correctly including the target variable and collapse factors if still needed. 
```

```{r}
#3 Check for missing variables and correct as needed.  
```

```{r}
#4 Split your data into test, tune, and train. (80/10/10)
```

```{r}
#5 Build your model using the training data, rpart2, and repeated cross validation as reviewed in class with the caret package.
```

```{r}
#6 View the results, comment on how the model performed and which variables appear to be contributing the most (variable importance)  
```

```{r}
#7 Plot the output of the model to see the tree visually, using rpart.plot, is there anything you notice that might be a concern?  
```

```{r}
#8 Use the tune set and the predict function with your model to make predicts for the target variable.
```

```{r}
#9 Use the postResample function to get your evaluation metrics. Also calculate NRMSE using the range (max-min) for the target variable. Explain what all these measures mean in terms of your models predictive power.  
```

```{r}
#10 Based on your understanding of the model and data adjust the hyper-parameter via the built in train control function in caret or build and try new features, does the model quality improve or not? If so how and why, if not, why not?

```

```{r}
#11 Once you are confident that your model is not improving, via changes implemented on the training set and evaluated on the the tune set, predict with the test set and report final evaluation of the model. Discuss the output in comparison with the previous evaluations.  
```

```{r}
#12 Summarize what you learned along the way and make recommendations on how this could be used moving forward, being careful not to over promise. 
```

```{r}
#13 What was the most interesting or hardest part of this process and what questions do you still have? 
```
